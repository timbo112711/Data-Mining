{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports for data-preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import for spliting the data set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Imports for classificaiton \n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('../data/master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [17, 34, 49, 64, 90]\n",
    "\n",
    "group_names = ['17-34', '35-49', '50-64', '65+']\n",
    "\n",
    "age_groups = pd.cut(df.age, bins, labels=group_names)\n",
    "df['age_groups'] = age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unwanted variables\n",
    "del df['workclass']\n",
    "del df['education']\n",
    "del df['education_num']\n",
    "del df['marital_status']\n",
    "del df['occupation']\n",
    "del df['native_country'] \n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric Selection\n",
    "\n",
    "Our team has used the sklearns.metrics.classification_report function in most cases to return an array of evaluation metrics. We expected initally that accuracy would be the most important for us. This is due to the nature of our prediction, which is guessing the age group and general income of an individual based on census data. The cost associated with incorrect prediction is low, so we can afford to build a model that focuses on accuracy. When we have a model that has the highest out of sample accuracy, it is theoretically leveraging generalized laws hidden within the data for the period trained. This value of this valuation type is determined by possible uses.\n",
    "\n",
    "Our prediction algorthims, which attempt to predict the age group of an individual or if an individual's income is greater or less than $50,000,  are primarly applicable to macroeconomic policy. While the abstract models are often well defined by economists, data collection is expensive. Even more so, pieccing together missing chunks is best done by extending pre-existing knowledge to fill the gaps. Prediction models like these have the primary goal of expanding a data set outwards so that it represents the reality. Accuracy is the most relevant here, with incorrect guesses  not being costly at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler -> Stratified Shuffle Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into traning (80%) and test set (20%)\n",
    "# We are using stratified cross validation here because the majority of the\n",
    "#    individuals in the variable race are white\n",
    "\n",
    "if 'income_binary' in df:\n",
    "    y = df['income_binary'].values #get values we need \n",
    "    del df['income_binary']        #get rid of the class label\n",
    "    X = df.values                  #use everything else to predict \n",
    "    \n",
    "X = pd.get_dummies(df).values\n",
    "\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)\n",
    "# Split the data into 20% Test and 80% Train\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.20, random_state=111)\n",
    "sss.get_n_splits(X, y) #retreving the splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [15163 16279 29246 ..., 21159 41049 17832] TEST: [43578  1917 17027 ..., 22163 39121  5217]\n",
      "TRAIN: [12894 40784 33210 ..., 40386 25846 36294] TEST: [22078 13800    49 ..., 31283 31586  5940]\n",
      "TRAIN: [16738 39693 30388 ...,   328 33912 39362] TEST: [24310 48705 25069 ..., 47258 14625 39292]\n",
      "TRAIN: [ 7391 39777 43398 ...,  8978 24399 34458] TEST: [ 8836  1328 27156 ..., 47164 10476 15648]\n",
      "TRAIN: [16863 33361 41054 ..., 26744 47828 11941] TEST: [18495 35842 20752 ..., 46535  4696 46808]\n",
      "TRAIN: [ 5743 21257 30549 ...,  5927  7506 19162] TEST: [21737 30911  7484 ..., 19717 27662 19780]\n",
      "TRAIN: [18797 40559 21393 ..., 47376 19268 42562] TEST: [13861 34766  4320 ..., 42723 30153 11994]\n",
      "TRAIN: [36977 11147 24500 ..., 40130 15262 22626] TEST: [13974 35810 43678 ...,  7735 14376 40480]\n",
      "TRAIN: [32752 37107 19197 ..., 26015 32870  7076] TEST: [36370 13966 29812 ..., 20543 23045 34324]\n",
      "TRAIN: [22441 41471 48039 ..., 32784  4446 45905] TEST: [  934  1627 37667 ...,  4950 44845 21169]\n"
     ]
    }
   ],
   "source": [
    "# Create a for loop that grabs the values for each fold for traing and test sets\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836933155901\n",
      "0.841641928549\n",
      "0.838263895998\n",
      "0.838059166752\n",
      "0.838878083734\n",
      "0.838059166752\n",
      "0.837956802129\n",
      "0.839492271471\n",
      "0.838775719111\n",
      "0.839185177603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.83693315590132056,\n",
       " 0.84164192854949327,\n",
       " 0.83826389599754325,\n",
       " 0.83805916675197056,\n",
       " 0.83887808373426143,\n",
       " 0.83805916675197056,\n",
       " 0.83795680212918411,\n",
       " 0.8394922714709796,\n",
       " 0.83877571911147508,\n",
       " 0.83918517760262057]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# criterion : 'gini' > 'entropy'\n",
    "# max_features : 0 -> 41 (default is fine)\n",
    "# bootstrap : keep True\n",
    "# max_depth : not material\n",
    "\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "iteration = 1\n",
    "scores = []\n",
    "for train_indices, test_indices in sss.split(X,y): \n",
    "    X_train = X[train_indices]  # train indices for X\n",
    "    y_train = y[train_indices]  # train indices for y\n",
    "    \n",
    "    X_test = X[test_indices]    # test indices for X\n",
    "    y_test = y[test_indices]    # test indices for y\n",
    "    \n",
    "    # train the reusable KNN classifier on the training data\n",
    "    RFC.fit(X_train,y_train)  # train object\n",
    "    y_hat = RFC.predict(X_test) # get test set precitions\n",
    "    \n",
    "    scores.append(mt.accuracy_score(y_hat,y_test))\n",
    "    print(mt.accuracy_score(y_hat,y_test))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "#Gaussian : .76\n",
    "#Bernoulli : .74\n",
    "BNB = BernoulliNB()\n",
    "\n",
    "iteration = 1\n",
    "\n",
    "for train_indices, test_indices in sss.split(X,y): \n",
    "    X_train = X[train_indices]  # train indices for X\n",
    "    y_train = y[train_indices]  # train indices for y\n",
    "    \n",
    "    X_test = X[test_indices]    # test indices for X\n",
    "    y_test = y[test_indices]    # test indices for y\n",
    "    \n",
    "    # train the reusable KNN classifier on the training data\n",
    "    BNB.fit(X_train,y_train)  # train object\n",
    "    y_hat = BNB.predict(X_test) # get test set precitions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84491759647865694"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trains slow and not accurate!\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC = SVC()\n",
    "\n",
    "iteration = 1\n",
    "scores = []\n",
    "for train_indices, test_indices in sss.split(X,y): \n",
    "    X_train = X[train_indices]  # train indices for X\n",
    "    y_train = y[train_indices]  # train indices for y\n",
    "    \n",
    "    X_test = X[test_indices]    # test indices for X\n",
    "    y_test = y[test_indices]    # test indices for y\n",
    "    \n",
    "    # train the reusable KNN classifier on the training data\n",
    "    SVC.fit(X_train,y_train)  # train object\n",
    "    y_hat = SVC.predict(X_test) # get test set precitions\n",
    "    \n",
    "    scores.append(mt.accuracy_score(y_hat,y_test))\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adjust Parameters \n",
    "\n",
    "We have seen above different algorithms perform with default hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# criterion : 'gini' > 'entropy'\n",
    "# max_features : 0 -> 41 (default is fine)\n",
    "# bootstrap : keep True\n",
    "# max_depth : not material\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "\n",
    "iteration = 1\n",
    "scores = []\n",
    "for train_indices, test_indices in sss.split(X,y): \n",
    "    X_train = X[train_indices]  # train indices for X\n",
    "    y_train = y[train_indices]  # train indices for y\n",
    "    \n",
    "    X_test = X[test_indices]    # test indices for X\n",
    "    y_test = y[test_indices]    # test indices for y\n",
    "    \n",
    "    # train the reusable KNN classifier on the training data\n",
    "    GBC.fit(X_train,y_train)  # train object\n",
    "    y_hat = GBC.predict(X_test) # get test set precitions\n",
    "    \n",
    "    scores.append(mt.accuracy_score(y_hat,y_test))\n",
    "\n",
    "np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'learning_rate':[.25,.5,1,2]}\n",
    "\n",
    "abc = GridSearchCV(AdaBoostClassifier(),params, cv=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#our data is already in a relatively useful shape\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 30000\n",
    "batch_size = 256\n",
    "\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_hidden_2 = 128 # 2nd layer num features (the latent dim)\n",
    "num_input = 45 # data shape [1,45]\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, _ = tf.train.batch(X_train,45)\n",
    "\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "    _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "    # Display logs per step\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        print('Step %i: Minibatch Loss: %f' % (i, l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
